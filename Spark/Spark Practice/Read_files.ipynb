{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"Deepu/names.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstNamesDF = df.select(\"firstName\", \"year\")\n",
    "firstNamesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstNamesDF.select(\"firstName\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 Popular Name in 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular = (df.filter(df['year'] == 1980).\n",
    "           filter(df['gender'] == 'F').\n",
    "           orderBy(df['total'].desc(), df['firstName']).\n",
    "           select(\"firstName\").\n",
    "           limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "lower = udf(lambda s:s.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssn2010 = df.filter(df['year'] == 2010).\\\n",
    "             select(df['total'].alias(\"total2010\"),\n",
    "                    df['gender'].alias(\"gender2010\"),\n",
    "                    df['firstName'].alias(\"firstName2010\"),\n",
    "                    lower(df['firstName']).alias(\"name2010\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssn1930 = df.filter(df['year'] == 1930).\\\n",
    "             select(df['total'].alias(\"total1930\"),   \n",
    "                    df['gender'].alias(\"gender1930\"),\n",
    "                    df['firstName'].alias(\"firstName1930\"), \n",
    "                    lower(df['firstName'].alias(\"name1930\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join table 2010 & 1930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = ssn2010.filter(ssn2010.gender2010 == \"F\").\\\n",
    "                 join(ssn1930.filter(ssn1930.gender1930 == \"F\"), ssn2010.name2010 == ssn1930.name1930).\\\n",
    "                 orderBy(ssn2010.total2010.desc()).\\\n",
    "                 limit(10).\\\n",
    "                 select(ssn2010.firstName2010.alias(\"name\"), ssn1930.total1930, ssn2010.total2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read xml files write the below statement in the shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark2 --master local --packages com.databricks:spark-xml_2.10:0.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read XML (not for all xml it will have changes according to references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = spark.read.format(\"com.databricks.spark.xml\").option(\"inferSchema\", \"true\").option(\"rootTag\", \"employees\").option(\"rowTag\", \"employee\").load(\"datasets/employees.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from RDBMS write the below statement in the shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark2 --master local --driver-class-path mysql-connector-java-5.1.46.jar --jars mysql-connector-java-5.1.46.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://mysqldb.projectpro.com:5147\") \\\n",
    "    .option(\"dbtable\", \"retail_db.order_items\") \\\n",
    "    .option(\"user\", \"labuser\") \\\n",
    "    .option(\"password\", \"DeepuPillai\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
